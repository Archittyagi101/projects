{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsihtu8JmcmQl6U48WHfUd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archittyagi101/projects/blob/main/Archit_ML_Lab_Manual_Archit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .CSV file. **"
      ],
      "metadata": {
        "id": "-UNEfuhJIwNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "79YsUVr8IoiU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(data=pd.read_csv('/content/ws.csv'))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rcpvqZT0NSbr",
        "outputId": "5de609e6-8708-4ca5-e115-2b974da7569a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sunny  Warm Normal  Strong Warm.1    Same  Yes\n",
            "0  Sunny  Warm   High  Strong   Warm    Same  Yes\n",
            "1  Rainy  Cold   High  Strong   Warm  Change   No\n",
            "2  Sunny  Warm   High  Strong   Cool  Change  Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_s_algorithm(training_data):\n",
        "    hypothesis = [\"0\"] * (len(training_data[0]) - 1)\n",
        "\n",
        "    for example in training_data:\n",
        "        if example[-1] == \"Yes\":\n",
        "            for i in range(len(example) - 1):\n",
        "                if hypothesis[i] == \"0\":\n",
        "                    hypothesis[i] = example[i]\n",
        "                elif hypothesis[i] != example[i]:\n",
        "                    hypothesis[i] = \"?\"\n",
        "\n",
        "    return hypothesis\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        data = [row for row in reader]\n",
        "    return data\n",
        "\n",
        "file_path = '/content/ws.csv'\n",
        "training_data = read_csv_file(file_path)\n",
        "hypothesis = find_s_algorithm(training_data)\n",
        "print(\"The most specific hypothesis is:\", hypothesis)\n",
        "\n"
      ],
      "metadata": {
        "id": "eAY_uf_BInpH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "86619ab2-0e9b-4fae-825e-c637ef701612"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most specific hypothesis is: ['Sunny', 'Warm', '?', 'Strong', '?', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.**"
      ],
      "metadata": {
        "id": "63YtM5fEJVjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concepts = np.array(data.iloc[:,0:-1])\n",
        "target = np.array(data.iloc[:,-1])\n",
        "def learn(concepts, target):\n",
        "\n",
        "    '''\n",
        "    learn() function implements the learning method of the Candidate elimination algorithm.\n",
        "    Arguments:\n",
        "        concepts - a data frame with all the features\n",
        "        target - a data frame with corresponding output values\n",
        "    '''\n",
        "\n",
        "    # Initialise S0 with the first instance from concepts\n",
        "    # .copy() makes sure a new list is created instead of just pointing to the same memory location\n",
        "    specific_h = concepts[0].copy()\n",
        "    print(\"\\nInitialization of specific_h and general_h\")\n",
        "    print(specific_h)\n",
        "    #h=[\"#\" for i in range(0,5)]\n",
        "    #print(h)\n",
        "\n",
        "    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n",
        "    print(general_h)\n",
        "    # The learning iterations\n",
        "    for i, h in enumerate(concepts):\n",
        "\n",
        "        # Checking if the hypothesis has a positive target\n",
        "        if target[i] == \"Yes\":\n",
        "            for x in range(len(specific_h)):\n",
        "\n",
        "                # Change values in S & G only if values change\n",
        "                if h[x] != specific_h[x]:\n",
        "                    specific_h[x] = '?'\n",
        "                    general_h[x][x] = '?'\n",
        "\n",
        "        # Checking if the hypothesis has a positive target\n",
        "        if target[i] == \"No\":\n",
        "            for x in range(len(specific_h)):\n",
        "                # For negative hyposthesis change values only  in G\n",
        "                if h[x] != specific_h[x]:\n",
        "                    general_h[x][x] = specific_h[x]\n",
        "                else:\n",
        "                    general_h[x][x] = '?'\n",
        "\n",
        "        print(\"\\nSteps of Candidate Elimination Algorithm\",i+1)\n",
        "        print(specific_h)\n",
        "        print(general_h)\n",
        "\n",
        "    # find indices where we have empty rows, meaning those that are unchanged\n",
        "    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]\n",
        "    for i in indices:\n",
        "        # remove those rows from general_h\n",
        "        general_h.remove(['?', '?', '?', '?', '?', '?'])\n",
        "    # Return final values\n",
        "    return specific_h, general_h\n",
        "\n",
        "s_final, g_final = learn(concepts, target)\n",
        "print(\"\\nFinal Specific_h:\", s_final, sep=\"\\n\")\n",
        "print(\"\\nFinal General_h:\", g_final, sep=\"\\n\")"
      ],
      "metadata": {
        "id": "q4i0xdckH2sk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b5e18c3d-b6d5-471b-d930-646d9e7046d9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initialization of specific_h and general_h\n",
            "['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
            "[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
            "\n",
            "Steps of Candidate Elimination Algorithm 1\n",
            "['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
            "[['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
            "\n",
            "Steps of Candidate Elimination Algorithm 2\n",
            "['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
            "[['Sunny', '?', '?', '?', '?', '?'], ['?', 'Warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', 'Same']]\n",
            "\n",
            "Steps of Candidate Elimination Algorithm 3\n",
            "['Sunny' 'Warm' 'High' 'Strong' '?' '?']\n",
            "[['Sunny', '?', '?', '?', '?', '?'], ['?', 'Warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
            "\n",
            "Final Specific_h:\n",
            "['Sunny' 'Warm' 'High' 'Strong' '?' '?']\n",
            "\n",
            "Final General_h:\n",
            "[['Sunny', '?', '?', '?', '?', '?'], ['?', 'Warm', '?', '?', '?', '?']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample. **"
      ],
      "metadata": {
        "id": "E6x1KfuKLoDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        datareader = csv.reader(csvfile, delimiter=',')\n",
        "        headers = next(datareader)\n",
        "        metadata = []\n",
        "        traindata = []\n",
        "        for name in headers:\n",
        "            metadata.append(name)\n",
        "        for row in datareader:\n",
        "            traindata.append(row)\n",
        "\n",
        "    return (metadata, traindata)\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute):\n",
        "        self.attribute = attribute\n",
        "        self.children = []\n",
        "        self.answer = \"\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.attribute\n",
        "\n",
        "def subtables(data, col, delete):\n",
        "    dict = {}\n",
        "    items = np.unique(data[:, col])\n",
        "    count = np.zeros((items.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    for x in range(items.shape[0]):\n",
        "        for y in range(data.shape[0]):\n",
        "            if data[y, col] == items[x]:\n",
        "                count[x] += 1\n",
        "\n",
        "    for x in range(items.shape[0]):\n",
        "        dict[items[x]] = np.empty((int(count[x]), data.shape[1]), dtype=\"|S32\")\n",
        "        pos = 0\n",
        "        for y in range(data.shape[0]):\n",
        "            if data[y, col] == items[x]:\n",
        "                dict[items[x]][pos] = data[y]\n",
        "                pos += 1\n",
        "        if delete:\n",
        "            dict[items[x]] = np.delete(dict[items[x]], col, 1)\n",
        "\n",
        "    return items, dict\n",
        "\n",
        "def entropy(S):\n",
        "    items = np.unique(S)\n",
        "\n",
        "    if items.size == 1:\n",
        "        return 0\n",
        "\n",
        "    counts = np.zeros((items.shape[0], 1))\n",
        "    sums = 0\n",
        "\n",
        "    for x in range(items.shape[0]):\n",
        "        counts[x] = sum(S == items[x]) / (S.size * 1.0)\n",
        "\n",
        "    for count in counts:\n",
        "        sums += -1 * count * math.log(count, 2)\n",
        "    return sums\n",
        "\n",
        "def gain_ratio(data, col):\n",
        "    items, dict = subtables(data, col, delete=False)\n",
        "\n",
        "    total_size = data.shape[0]\n",
        "    entropies = np.zeros((items.shape[0], 1))\n",
        "    intrinsic = np.zeros((items.shape[0], 1))\n",
        "\n",
        "    for x in range(items.shape[0]):\n",
        "        ratio = dict[items[x]].shape[0]/(total_size * 1.0)\n",
        "        entropies[x] = ratio * entropy(dict[items[x]][:, -1])\n",
        "        intrinsic[x] = ratio * math.log(ratio, 2)\n",
        "\n",
        "    total_entropy = entropy(data[:, -1])\n",
        "    iv = -1 * sum(intrinsic)\n",
        "\n",
        "    for x in range(entropies.shape[0]):\n",
        "        total_entropy -= entropies[x]\n",
        "\n",
        "    return total_entropy / iv\n",
        "\n",
        "def create_node(data, metadata):\n",
        "    if (np.unique(data[:, -1])).shape[0] == 1:\n",
        "        node = Node(\"\")\n",
        "        node.answer = np.unique(data[:, -1])[0]\n",
        "        return node\n",
        "\n",
        "    gains = np.zeros((data.shape[1] - 1, 1))\n",
        "\n",
        "    for col in range(data.shape[1] - 1):\n",
        "        gains[col] = gain_ratio(data, col)\n",
        "\n",
        "    split = np.argmax(gains)\n",
        "\n",
        "    node = Node(metadata[split])\n",
        "    metadata = np.delete(metadata, split, 0)\n",
        "\n",
        "    items, dict = subtables(data, split, delete=True)\n",
        "\n",
        "    for x in range(items.shape[0]):\n",
        "        child = create_node(dict[items[x]], metadata)\n",
        "        node.children.append((items[x], child))\n",
        "\n",
        "    return node\n",
        "\n",
        "def empty(size):\n",
        "    s = \"\"\n",
        "    for x in range(size):\n",
        "        s += \"   \"\n",
        "    return s\n",
        "\n",
        "def print_tree(node, level):\n",
        "    if node.answer != \"\":\n",
        "        print(empty(level), node.answer)\n",
        "        return\n",
        "    print(empty(level), node.attribute)\n",
        "    for value, n in node.children:\n",
        "        print(empty(level + 1), value)\n",
        "        print_tree(n, level + 2)\n",
        "\n",
        "metadata, traindata = read_data(\"/content/tennisdata.csv\")\n",
        "data = np.array(traindata)\n",
        "node = create_node(data, metadata)\n",
        "print_tree(node, 0)"
      ],
      "metadata": {
        "id": "XV42IighH2uE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6a38a01a-22fb-461b-fa31-775dc01728a6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Outlook\n",
            "    Overcast\n",
            "       b'Yes'\n",
            "    Rainy\n",
            "       Windy\n",
            "          b'False'\n",
            "             b'Yes'\n",
            "          b'True'\n",
            "             b'No'\n",
            "    Sunny\n",
            "       Humidity\n",
            "          b'High'\n",
            "             b'No'\n",
            "          b'Normal'\n",
            "             b'Yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets.**"
      ],
      "metadata": {
        "id": "BYEq_wtpPZ3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)     # X = (hours sleeping, hours studying)\n",
        "y = np.array(([92], [86], [89]), dtype=float)           # y = score on test\n",
        "\n",
        "# scale units\n",
        "X = X/np.amax(X, axis=0)        # maximum of X array\n",
        "y = y/100                       # max test score is 100\n",
        "\n",
        "class Neural_Network(object):\n",
        "    def __init__(self):\n",
        "                            # Parameters\n",
        "        self.inputSize = 2\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 3\n",
        "                             # Weights\n",
        "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize)        # (3x2) weight matrix from input to hidden layer\n",
        "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize)       # (3x1) weight matrix from hidden to output layer\n",
        "\n",
        "    def forward(self, X):\n",
        "                             #forward propagation through our network\n",
        "        self.z = np.dot(X, self.W1)               # dot product of X (input) and first set of 3x2 weights\n",
        "        self.z2 = self.sigmoid(self.z)            # activation function\n",
        "        self.z3 = np.dot(self.z2, self.W2)        # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "        o = self.sigmoid(self.z3)                 # final activation function\n",
        "        return o\n",
        "\n",
        "    def sigmoid(self, s):\n",
        "        return 1/(1+np.exp(-s))     # activation function\n",
        "\n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)          # derivative of sigmoid\n",
        "\n",
        "    def backward(self, X, y, o):\n",
        "                                    # backward propgate through the network\n",
        "        self.o_error = y - o        # error in output\n",
        "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to\n",
        "        self.z2_error = self.o_delta.dot(self.W2.T)    # z2 error: how much our hidden layer weights contributed to output error\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
        "        self.W1 += X.T.dot(self.z2_delta)       # adjusting first set (input --> hidden) weights\n",
        "        self.W2 += self.z2.T.dot(self.o_delta)  # adjusting second set (hidden --> output) weights\n",
        "\n",
        "    def train (self, X, y):\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n",
        "\n",
        "\n",
        "NN = Neural_Network()\n",
        "for i in range(1000): # trains the NN 1,000 times\n",
        "    print (\"\\nInput: \\n\" + str(X))\n",
        "    print (\"\\nActual Output: \\n\" + str(y))\n",
        "    print (\"\\nPredicted Output: \\n\" + str(NN.forward(X)))\n",
        "    print (\"\\nLoss: \\n\" + str(np.mean(np.square(y - NN.forward(X)))))     # mean sum squared loss)\n",
        "    NN.train(X, y)"
      ],
      "metadata": {
        "id": "lgqGFtLRH2xL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "915e4cc6-3332-4b7d-c82c-85cb166b86cc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535868]\n",
            " [0.88001388]\n",
            " [0.88458787]]\n",
            "\n",
            "Loss: \n",
            "0.00021473820159906778\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.905358  ]\n",
            " [0.88000879]\n",
            " [0.88459235]]\n",
            "\n",
            "Loss: \n",
            "0.0002146608573693035\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535731]\n",
            " [0.88000371]\n",
            " [0.88459683]]\n",
            "\n",
            "Loss: \n",
            "0.00021458360670400506\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535662]\n",
            " [0.87999863]\n",
            " [0.88460131]]\n",
            "\n",
            "Loss: \n",
            "0.00021450644945881258\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535594]\n",
            " [0.87999355]\n",
            " [0.88460578]]\n",
            "\n",
            "Loss: \n",
            "0.00021442938548962473\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535526]\n",
            " [0.87998848]\n",
            " [0.88461025]]\n",
            "\n",
            "Loss: \n",
            "0.00021435241465261052\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535457]\n",
            " [0.87998341]\n",
            " [0.88461472]]\n",
            "\n",
            "Loss: \n",
            "0.00021427553680419698\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535389]\n",
            " [0.87997835]\n",
            " [0.88461918]]\n",
            "\n",
            "Loss: \n",
            "0.00021419875180105942\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535321]\n",
            " [0.87997329]\n",
            " [0.88462364]]\n",
            "\n",
            "Loss: \n",
            "0.00021412205950015274\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535253]\n",
            " [0.87996824]\n",
            " [0.88462809]]\n",
            "\n",
            "Loss: \n",
            "0.00021404545975867086\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535185]\n",
            " [0.87996319]\n",
            " [0.88463254]]\n",
            "\n",
            "Loss: \n",
            "0.00021396895243407254\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535117]\n",
            " [0.87995814]\n",
            " [0.88463699]]\n",
            "\n",
            "Loss: \n",
            "0.00021389253738408195\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90535049]\n",
            " [0.8799531 ]\n",
            " [0.88464143]]\n",
            "\n",
            "Loss: \n",
            "0.00021381621446666603\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534982]\n",
            " [0.87994807]\n",
            " [0.88464588]]\n",
            "\n",
            "Loss: \n",
            "0.00021373998354006475\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534914]\n",
            " [0.87994304]\n",
            " [0.88465031]]\n",
            "\n",
            "Loss: \n",
            "0.0002136638444627572\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534846]\n",
            " [0.87993801]\n",
            " [0.88465475]]\n",
            "\n",
            "Loss: \n",
            "0.00021358779709349155\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534779]\n",
            " [0.87993298]\n",
            " [0.88465918]]\n",
            "\n",
            "Loss: \n",
            "0.000213511841291263\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534712]\n",
            " [0.87992797]\n",
            " [0.8846636 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002134359769153228\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534644]\n",
            " [0.87992295]\n",
            " [0.88466803]]\n",
            "\n",
            "Loss: \n",
            "0.00021336020382518089\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534577]\n",
            " [0.87991794]\n",
            " [0.88467245]]\n",
            "\n",
            "Loss: \n",
            "0.00021328452188059492\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9053451 ]\n",
            " [0.87991293]\n",
            " [0.88467686]]\n",
            "\n",
            "Loss: \n",
            "0.00021320893094157483\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534443]\n",
            " [0.87990793]\n",
            " [0.88468128]]\n",
            "\n",
            "Loss: \n",
            "0.00021313343086838918\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534376]\n",
            " [0.87990294]\n",
            " [0.88468569]]\n",
            "\n",
            "Loss: \n",
            "0.00021305802152155582\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534309]\n",
            " [0.87989794]\n",
            " [0.88469009]]\n",
            "\n",
            "Loss: \n",
            "0.00021298270276183997\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534243]\n",
            " [0.87989295]\n",
            " [0.8846945 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002129074744502649\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534176]\n",
            " [0.87988797]\n",
            " [0.8846989 ]]\n",
            "\n",
            "Loss: \n",
            "0.00021283233644809208\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534109]\n",
            " [0.87988299]\n",
            " [0.88470329]]\n",
            "\n",
            "Loss: \n",
            "0.00021275728861685125\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90534043]\n",
            " [0.87987801]\n",
            " [0.88470769]]\n",
            "\n",
            "Loss: \n",
            "0.0002126823308183068\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533976]\n",
            " [0.87987304]\n",
            " [0.88471207]]\n",
            "\n",
            "Loss: \n",
            "0.00021260746291447676\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9053391 ]\n",
            " [0.87986807]\n",
            " [0.88471646]]\n",
            "\n",
            "Loss: \n",
            "0.00021253268476762677\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533844]\n",
            " [0.87986311]\n",
            " [0.88472084]]\n",
            "\n",
            "Loss: \n",
            "0.00021245799624027248\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533778]\n",
            " [0.87985815]\n",
            " [0.88472522]]\n",
            "\n",
            "Loss: \n",
            "0.00021238339719517716\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533712]\n",
            " [0.8798532 ]\n",
            " [0.8847296 ]]\n",
            "\n",
            "Loss: \n",
            "0.00021230888749534257\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533646]\n",
            " [0.87984825]\n",
            " [0.88473397]]\n",
            "\n",
            "Loss: \n",
            "0.00021223446700403\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9053358 ]\n",
            " [0.8798433 ]\n",
            " [0.88473834]]\n",
            "\n",
            "Loss: \n",
            "0.00021216013558473772\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533514]\n",
            " [0.87983836]\n",
            " [0.8847427 ]]\n",
            "\n",
            "Loss: \n",
            "0.000212085893101213\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533448]\n",
            " [0.87983342]\n",
            " [0.88474707]]\n",
            "\n",
            "Loss: \n",
            "0.0002120117394174449\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533383]\n",
            " [0.87982849]\n",
            " [0.88475142]]\n",
            "\n",
            "Loss: \n",
            "0.00021193767439767417\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533317]\n",
            " [0.87982356]\n",
            " [0.88475578]]\n",
            "\n",
            "Loss: \n",
            "0.0002118636979063732\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533252]\n",
            " [0.87981863]\n",
            " [0.88476013]]\n",
            "\n",
            "Loss: \n",
            "0.00021178980980827232\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533186]\n",
            " [0.87981371]\n",
            " [0.88476448]]\n",
            "\n",
            "Loss: \n",
            "0.00021171600996832847\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533121]\n",
            " [0.87980879]\n",
            " [0.88476882]]\n",
            "\n",
            "Loss: \n",
            "0.0002116422982517609\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90533056]\n",
            " [0.87980388]\n",
            " [0.88477317]]\n",
            "\n",
            "Loss: \n",
            "0.00021156867452400922\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532991]\n",
            " [0.87979897]\n",
            " [0.8847775 ]]\n",
            "\n",
            "Loss: \n",
            "0.00021149513865077125\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532925]\n",
            " [0.87979407]\n",
            " [0.88478184]]\n",
            "\n",
            "Loss: \n",
            "0.00021142169049798233\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9053286 ]\n",
            " [0.87978917]\n",
            " [0.88478617]]\n",
            "\n",
            "Loss: \n",
            "0.00021134832993180498\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532796]\n",
            " [0.87978427]\n",
            " [0.8847905 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002112750568186621\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532731]\n",
            " [0.87977938]\n",
            " [0.88479482]]\n",
            "\n",
            "Loss: \n",
            "0.00021120187102520712\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532666]\n",
            " [0.87977449]\n",
            " [0.88479915]]\n",
            "\n",
            "Loss: \n",
            "0.00021112877241832707\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532601]\n",
            " [0.87976961]\n",
            " [0.88480346]]\n",
            "\n",
            "Loss: \n",
            "0.00021105576086515314\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532537]\n",
            " [0.87976473]\n",
            " [0.88480778]]\n",
            "\n",
            "Loss: \n",
            "0.00021098283623305553\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532472]\n",
            " [0.87975985]\n",
            " [0.88481209]]\n",
            "\n",
            "Loss: \n",
            "0.0002109099983896401\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532408]\n",
            " [0.87975498]\n",
            " [0.8848164 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002108372472027513\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532344]\n",
            " [0.87975012]\n",
            " [0.8848207 ]]\n",
            "\n",
            "Loss: \n",
            "0.00021076458254046726\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532279]\n",
            " [0.87974525]\n",
            " [0.884825  ]]\n",
            "\n",
            "Loss: \n",
            "0.00021069200427110597\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532215]\n",
            " [0.8797404 ]\n",
            " [0.8848293 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002106195122632186\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532151]\n",
            " [0.87973554]\n",
            " [0.8848336 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002105471063855917\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532087]\n",
            " [0.87973069]\n",
            " [0.88483789]]\n",
            "\n",
            "Loss: \n",
            "0.00021047478650725143\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90532023]\n",
            " [0.87972584]\n",
            " [0.88484218]]\n",
            "\n",
            "Loss: \n",
            "0.00021040255249744577\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531959]\n",
            " [0.879721  ]\n",
            " [0.88484646]]\n",
            "\n",
            "Loss: \n",
            "0.00021033040422567369\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531896]\n",
            " [0.87971616]\n",
            " [0.88485074]]\n",
            "\n",
            "Loss: \n",
            "0.0002102583415616541\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531832]\n",
            " [0.87971133]\n",
            " [0.88485502]]\n",
            "\n",
            "Loss: \n",
            "0.00021018636437534336\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531768]\n",
            " [0.8797065 ]\n",
            " [0.8848593 ]]\n",
            "\n",
            "Loss: \n",
            "0.00021011447253693389\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531705]\n",
            " [0.87970168]\n",
            " [0.88486357]]\n",
            "\n",
            "Loss: \n",
            "0.00021004266591684211\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531641]\n",
            " [0.87969685]\n",
            " [0.88486784]]\n",
            "\n",
            "Loss: \n",
            "0.00020997094438572478\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531578]\n",
            " [0.87969204]\n",
            " [0.8848721 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020989930781446274\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531515]\n",
            " [0.87968722]\n",
            " [0.88487636]]\n",
            "\n",
            "Loss: \n",
            "0.00020982775607416906\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531452]\n",
            " [0.87968241]\n",
            " [0.88488062]]\n",
            "\n",
            "Loss: \n",
            "0.00020975628903619433\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531389]\n",
            " [0.87967761]\n",
            " [0.88488488]]\n",
            "\n",
            "Loss: \n",
            "0.00020968490657210376\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531326]\n",
            " [0.87967281]\n",
            " [0.88488913]]\n",
            "\n",
            "Loss: \n",
            "0.00020961360855370142\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531263]\n",
            " [0.87966801]\n",
            " [0.88489338]]\n",
            "\n",
            "Loss: \n",
            "0.00020954239485302916\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.905312  ]\n",
            " [0.87966322]\n",
            " [0.88489762]]\n",
            "\n",
            "Loss: \n",
            "0.00020947126534233515\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531137]\n",
            " [0.87965843]\n",
            " [0.88490186]]\n",
            "\n",
            "Loss: \n",
            "0.0002094002198941108\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531074]\n",
            " [0.87965364]\n",
            " [0.8849061 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020932925838107377\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90531012]\n",
            " [0.87964886]\n",
            " [0.88491034]]\n",
            "\n",
            "Loss: \n",
            "0.00020925838067616464\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530949]\n",
            " [0.87964409]\n",
            " [0.88491457]]\n",
            "\n",
            "Loss: \n",
            "0.0002091875866525538\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530887]\n",
            " [0.87963931]\n",
            " [0.8849188 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002091168761836322\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530824]\n",
            " [0.87963454]\n",
            " [0.88492302]]\n",
            "\n",
            "Loss: \n",
            "0.00020904624914302354\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530762]\n",
            " [0.87962978]\n",
            " [0.88492725]]\n",
            "\n",
            "Loss: \n",
            "0.00020897570540456725\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.905307  ]\n",
            " [0.87962502]\n",
            " [0.88493146]]\n",
            "\n",
            "Loss: \n",
            "0.00020890524484234313\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530638]\n",
            " [0.87962026]\n",
            " [0.88493568]]\n",
            "\n",
            "Loss: \n",
            "0.00020883486733063667\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530575]\n",
            " [0.87961551]\n",
            " [0.88493989]]\n",
            "\n",
            "Loss: \n",
            "0.00020876457274397015\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530513]\n",
            " [0.87961076]\n",
            " [0.8849441 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020869436095708233\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530452]\n",
            " [0.87960602]\n",
            " [0.88494831]]\n",
            "\n",
            "Loss: \n",
            "0.00020862423184493878\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9053039 ]\n",
            " [0.87960127]\n",
            " [0.88495251]]\n",
            "\n",
            "Loss: \n",
            "0.00020855418528272873\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530328]\n",
            " [0.87959654]\n",
            " [0.88495671]]\n",
            "\n",
            "Loss: \n",
            "0.00020848422114585046\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530266]\n",
            " [0.87959181]\n",
            " [0.88496091]]\n",
            "\n",
            "Loss: \n",
            "0.00020841433930994787\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530205]\n",
            " [0.87958708]\n",
            " [0.8849651 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002083445396508617\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530143]\n",
            " [0.87958235]\n",
            " [0.88496929]]\n",
            "\n",
            "Loss: \n",
            "0.00020827482204466733\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90530082]\n",
            " [0.87957763]\n",
            " [0.88497348]]\n",
            "\n",
            "Loss: \n",
            "0.00020820518636765445\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9053002 ]\n",
            " [0.87957291]\n",
            " [0.88497766]]\n",
            "\n",
            "Loss: \n",
            "0.00020813563249633964\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529959]\n",
            " [0.8795682 ]\n",
            " [0.88498184]]\n",
            "\n",
            "Loss: \n",
            "0.00020806616030744996\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529898]\n",
            " [0.87956349]\n",
            " [0.88498602]]\n",
            "\n",
            "Loss: \n",
            "0.00020799676967793633\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529837]\n",
            " [0.87955879]\n",
            " [0.88499019]]\n",
            "\n",
            "Loss: \n",
            "0.0002079274604849681\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529776]\n",
            " [0.87955409]\n",
            " [0.88499436]]\n",
            "\n",
            "Loss: \n",
            "0.00020785823260593007\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529715]\n",
            " [0.87954939]\n",
            " [0.88499853]]\n",
            "\n",
            "Loss: \n",
            "0.0002077890859184254\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529654]\n",
            " [0.8795447 ]\n",
            " [0.88500269]]\n",
            "\n",
            "Loss: \n",
            "0.00020772002030027938\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529593]\n",
            " [0.87954001]\n",
            " [0.88500685]]\n",
            "\n",
            "Loss: \n",
            "0.00020765103562952366\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529532]\n",
            " [0.87953532]\n",
            " [0.88501101]]\n",
            "\n",
            "Loss: \n",
            "0.00020758213178441584\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529471]\n",
            " [0.87953064]\n",
            " [0.88501517]]\n",
            "\n",
            "Loss: \n",
            "0.00020751330864342537\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529411]\n",
            " [0.87952596]\n",
            " [0.88501932]]\n",
            "\n",
            "Loss: \n",
            "0.0002074445660852403\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052935 ]\n",
            " [0.87952129]\n",
            " [0.88502347]]\n",
            "\n",
            "Loss: \n",
            "0.0002073759039887595\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052929 ]\n",
            " [0.87951662]\n",
            " [0.88502761]]\n",
            "\n",
            "Loss: \n",
            "0.0002073073222330931\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052923 ]\n",
            " [0.87951196]\n",
            " [0.88503175]]\n",
            "\n",
            "Loss: \n",
            "0.00020723882069757617\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529169]\n",
            " [0.87950729]\n",
            " [0.88503589]]\n",
            "\n",
            "Loss: \n",
            "0.0002071703992617495\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529109]\n",
            " [0.87950264]\n",
            " [0.88504003]]\n",
            "\n",
            "Loss: \n",
            "0.00020710205780537024\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90529049]\n",
            " [0.87949798]\n",
            " [0.88504416]]\n",
            "\n",
            "Loss: \n",
            "0.0002070337962084034\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528989]\n",
            " [0.87949333]\n",
            " [0.88504829]]\n",
            "\n",
            "Loss: \n",
            "0.00020696561435103262\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528929]\n",
            " [0.87948869]\n",
            " [0.88505241]]\n",
            "\n",
            "Loss: \n",
            "0.00020689751211365543\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528869]\n",
            " [0.87948404]\n",
            " [0.88505654]]\n",
            "\n",
            "Loss: \n",
            "0.00020682948937687033\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528809]\n",
            " [0.87947941]\n",
            " [0.88506066]]\n",
            "\n",
            "Loss: \n",
            "0.00020676154602149578\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528749]\n",
            " [0.87947477]\n",
            " [0.88506477]]\n",
            "\n",
            "Loss: \n",
            "0.00020669368192856088\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052869 ]\n",
            " [0.87947014]\n",
            " [0.88506889]]\n",
            "\n",
            "Loss: \n",
            "0.0002066258969792991\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052863 ]\n",
            " [0.87946551]\n",
            " [0.885073  ]]\n",
            "\n",
            "Loss: \n",
            "0.00020655819105516086\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052857 ]\n",
            " [0.87946089]\n",
            " [0.8850771 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020649056403780383\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528511]\n",
            " [0.87945627]\n",
            " [0.88508121]]\n",
            "\n",
            "Loss: \n",
            "0.00020642301580908824\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528452]\n",
            " [0.87945166]\n",
            " [0.88508531]]\n",
            "\n",
            "Loss: \n",
            "0.0002063555462510911\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528392]\n",
            " [0.87944705]\n",
            " [0.88508941]]\n",
            "\n",
            "Loss: \n",
            "0.00020628815524609818\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528333]\n",
            " [0.87944244]\n",
            " [0.8850935 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020622084267659302\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528274]\n",
            " [0.87943784]\n",
            " [0.88509759]]\n",
            "\n",
            "Loss: \n",
            "0.00020615360842528567\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528215]\n",
            " [0.87943324]\n",
            " [0.88510168]]\n",
            "\n",
            "Loss: \n",
            "0.00020608645237506763\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528156]\n",
            " [0.87942864]\n",
            " [0.88510577]]\n",
            "\n",
            "Loss: \n",
            "0.00020601937440905586\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528097]\n",
            " [0.87942405]\n",
            " [0.88510985]]\n",
            "\n",
            "Loss: \n",
            "0.00020595237441056857\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90528038]\n",
            " [0.87941946]\n",
            " [0.88511393]]\n",
            "\n",
            "Loss: \n",
            "0.00020588545226313119\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527979]\n",
            " [0.87941488]\n",
            " [0.885118  ]]\n",
            "\n",
            "Loss: \n",
            "0.00020581860785046874\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052792 ]\n",
            " [0.8794103 ]\n",
            " [0.88512208]]\n",
            "\n",
            "Loss: \n",
            "0.0002057518410565164\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527862]\n",
            " [0.87940572]\n",
            " [0.88512614]]\n",
            "\n",
            "Loss: \n",
            "0.0002056851517654141\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527803]\n",
            " [0.87940115]\n",
            " [0.88513021]]\n",
            "\n",
            "Loss: \n",
            "0.0002056185398615001\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527745]\n",
            " [0.87939658]\n",
            " [0.88513427]]\n",
            "\n",
            "Loss: \n",
            "0.00020555200522932718\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527686]\n",
            " [0.87939201]\n",
            " [0.88513833]]\n",
            "\n",
            "Loss: \n",
            "0.00020548554775364026\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527628]\n",
            " [0.87938745]\n",
            " [0.88514239]]\n",
            "\n",
            "Loss: \n",
            "0.00020541916731939397\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052757 ]\n",
            " [0.87938289]\n",
            " [0.88514645]]\n",
            "\n",
            "Loss: \n",
            "0.00020535286381174092\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527511]\n",
            " [0.87937834]\n",
            " [0.8851505 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020528663711604097\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527453]\n",
            " [0.87937379]\n",
            " [0.88515454]]\n",
            "\n",
            "Loss: \n",
            "0.00020522048711785212\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527395]\n",
            " [0.87936924]\n",
            " [0.88515859]]\n",
            "\n",
            "Loss: \n",
            "0.00020515441370293612\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527337]\n",
            " [0.8793647 ]\n",
            " [0.88516263]]\n",
            "\n",
            "Loss: \n",
            "0.00020508841675725143\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527279]\n",
            " [0.87936016]\n",
            " [0.88516667]]\n",
            "\n",
            "Loss: \n",
            "0.00020502249616696203\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527222]\n",
            " [0.87935563]\n",
            " [0.8851707 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020495665181842872\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527164]\n",
            " [0.87935109]\n",
            " [0.88517474]]\n",
            "\n",
            "Loss: \n",
            "0.00020489088359821423\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527106]\n",
            " [0.87934657]\n",
            " [0.88517877]]\n",
            "\n",
            "Loss: \n",
            "0.00020482519139307723\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90527048]\n",
            " [0.87934204]\n",
            " [0.88518279]]\n",
            "\n",
            "Loss: \n",
            "0.0002047595750899792\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526991]\n",
            " [0.87933752]\n",
            " [0.88518682]]\n",
            "\n",
            "Loss: \n",
            "0.00020469403457608112\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526933]\n",
            " [0.87933301]\n",
            " [0.88519084]]\n",
            "\n",
            "Loss: \n",
            "0.00020462856973873755\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526876]\n",
            " [0.87932849]\n",
            " [0.88519485]]\n",
            "\n",
            "Loss: \n",
            "0.0002045631804655033\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526819]\n",
            " [0.87932399]\n",
            " [0.88519887]]\n",
            "\n",
            "Loss: \n",
            "0.00020449786664412798\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526761]\n",
            " [0.87931948]\n",
            " [0.88520288]]\n",
            "\n",
            "Loss: \n",
            "0.00020443262816256412\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526704]\n",
            " [0.87931498]\n",
            " [0.88520689]]\n",
            "\n",
            "Loss: \n",
            "0.00020436746490895436\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526647]\n",
            " [0.87931048]\n",
            " [0.88521089]]\n",
            "\n",
            "Loss: \n",
            "0.00020430237677164606\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052659 ]\n",
            " [0.87930599]\n",
            " [0.88521489]]\n",
            "\n",
            "Loss: \n",
            "0.00020423736363916881\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526533]\n",
            " [0.8793015 ]\n",
            " [0.88521889]]\n",
            "\n",
            "Loss: \n",
            "0.00020417242540026013\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526476]\n",
            " [0.87929701]\n",
            " [0.88522289]]\n",
            "\n",
            "Loss: \n",
            "0.0002041075619438472\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526419]\n",
            " [0.87929253]\n",
            " [0.88522688]]\n",
            "\n",
            "Loss: \n",
            "0.0002040427731590538\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526363]\n",
            " [0.87928805]\n",
            " [0.88523087]]\n",
            "\n",
            "Loss: \n",
            "0.00020397805893519348\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526306]\n",
            " [0.87928357]\n",
            " [0.88523486]]\n",
            "\n",
            "Loss: \n",
            "0.0002039134191617821\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526249]\n",
            " [0.8792791 ]\n",
            " [0.88523884]]\n",
            "\n",
            "Loss: \n",
            "0.00020384885372852352\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526193]\n",
            " [0.87927464]\n",
            " [0.88524282]]\n",
            "\n",
            "Loss: \n",
            "0.0002037843625253111\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526136]\n",
            " [0.87927017]\n",
            " [0.8852468 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020371994544223927\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052608 ]\n",
            " [0.87926571]\n",
            " [0.88525077]]\n",
            "\n",
            "Loss: \n",
            "0.0002036556023695861\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90526024]\n",
            " [0.87926125]\n",
            " [0.88525474]]\n",
            "\n",
            "Loss: \n",
            "0.00020359133319783262\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525967]\n",
            " [0.8792568 ]\n",
            " [0.88525871]]\n",
            "\n",
            "Loss: \n",
            "0.00020352713781763917\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525911]\n",
            " [0.87925235]\n",
            " [0.88526268]]\n",
            "\n",
            "Loss: \n",
            "0.00020346301611986822\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525855]\n",
            " [0.87924791]\n",
            " [0.88526664]]\n",
            "\n",
            "Loss: \n",
            "0.00020339896799556685\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525799]\n",
            " [0.87924346]\n",
            " [0.8852706 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002033349933359695\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525743]\n",
            " [0.87923902]\n",
            " [0.88527456]]\n",
            "\n",
            "Loss: \n",
            "0.00020327109203251162\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525687]\n",
            " [0.87923459]\n",
            " [0.88527851]]\n",
            "\n",
            "Loss: \n",
            "0.00020320726397680752\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525631]\n",
            " [0.87923016]\n",
            " [0.88528246]]\n",
            "\n",
            "Loss: \n",
            "0.00020314350906067006\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525575]\n",
            " [0.87922573]\n",
            " [0.88528641]]\n",
            "\n",
            "Loss: \n",
            "0.00020307982717609205\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052552 ]\n",
            " [0.87922131]\n",
            " [0.88529035]]\n",
            "\n",
            "Loss: \n",
            "0.00020301621821525932\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525464]\n",
            " [0.87921689]\n",
            " [0.88529429]]\n",
            "\n",
            "Loss: \n",
            "0.00020295268207055106\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525408]\n",
            " [0.87921247]\n",
            " [0.88529823]]\n",
            "\n",
            "Loss: \n",
            "0.00020288921863452315\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525353]\n",
            " [0.87920806]\n",
            " [0.88530217]]\n",
            "\n",
            "Loss: \n",
            "0.0002028258277999276\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525298]\n",
            " [0.87920365]\n",
            " [0.8853061 ]]\n",
            "\n",
            "Loss: \n",
            "0.0002027625094596982\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525242]\n",
            " [0.87919924]\n",
            " [0.88531003]]\n",
            "\n",
            "Loss: \n",
            "0.00020269926350696343\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525187]\n",
            " [0.87919484]\n",
            " [0.88531396]]\n",
            "\n",
            "Loss: \n",
            "0.00020263608983503214\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525132]\n",
            " [0.87919044]\n",
            " [0.88531788]]\n",
            "\n",
            "Loss: \n",
            "0.00020257298833739254\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525077]\n",
            " [0.87918605]\n",
            " [0.8853218 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020250995890773343\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90525021]\n",
            " [0.87918166]\n",
            " [0.88532572]]\n",
            "\n",
            "Loss: \n",
            "0.00020244700143992126\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524966]\n",
            " [0.87917727]\n",
            " [0.88532963]]\n",
            "\n",
            "Loss: \n",
            "0.0002023841158280069\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524911]\n",
            " [0.87917288]\n",
            " [0.88533355]]\n",
            "\n",
            "Loss: \n",
            "0.0002023213019662225\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524857]\n",
            " [0.8791685 ]\n",
            " [0.88533745]]\n",
            "\n",
            "Loss: \n",
            "0.0002022585597489916\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524802]\n",
            " [0.87916413]\n",
            " [0.88534136]]\n",
            "\n",
            "Loss: \n",
            "0.00020219588907092003\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524747]\n",
            " [0.87915975]\n",
            " [0.88534526]]\n",
            "\n",
            "Loss: \n",
            "0.00020213328982679674\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524692]\n",
            " [0.87915538]\n",
            " [0.88534916]]\n",
            "\n",
            "Loss: \n",
            "0.0002020707619115883\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524638]\n",
            " [0.87915102]\n",
            " [0.88535306]]\n",
            "\n",
            "Loss: \n",
            "0.00020200830522044914\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524583]\n",
            " [0.87914666]\n",
            " [0.88535695]]\n",
            "\n",
            "Loss: \n",
            "0.00020194591964871548\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524529]\n",
            " [0.8791423 ]\n",
            " [0.88536085]]\n",
            "\n",
            "Loss: \n",
            "0.00020188360509190562\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524474]\n",
            " [0.87913794]\n",
            " [0.88536473]]\n",
            "\n",
            "Loss: \n",
            "0.00020182136144572145\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052442 ]\n",
            " [0.87913359]\n",
            " [0.88536862]]\n",
            "\n",
            "Loss: \n",
            "0.00020175918860604187\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524366]\n",
            " [0.87912924]\n",
            " [0.8853725 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020169708646893212\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524312]\n",
            " [0.8791249 ]\n",
            " [0.88537638]]\n",
            "\n",
            "Loss: \n",
            "0.0002016350549306302\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524257]\n",
            " [0.87912056]\n",
            " [0.88538026]]\n",
            "\n",
            "Loss: \n",
            "0.00020157309388756084\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524203]\n",
            " [0.87911622]\n",
            " [0.88538413]]\n",
            "\n",
            "Loss: \n",
            "0.0002015112032363298\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524149]\n",
            " [0.87911188]\n",
            " [0.885388  ]]\n",
            "\n",
            "Loss: \n",
            "0.0002014493828737177\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524095]\n",
            " [0.87910755]\n",
            " [0.88539187]]\n",
            "\n",
            "Loss: \n",
            "0.00020138763269668457\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90524042]\n",
            " [0.87910323]\n",
            " [0.88539573]]\n",
            "\n",
            "Loss: \n",
            "0.00020132595260237226\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523988]\n",
            " [0.8790989 ]\n",
            " [0.8853996 ]]\n",
            "\n",
            "Loss: \n",
            "0.00020126434248809985\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523934]\n",
            " [0.87909458]\n",
            " [0.88540346]]\n",
            "\n",
            "Loss: \n",
            "0.0002012028022513641\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052388 ]\n",
            " [0.87909027]\n",
            " [0.88540731]]\n",
            "\n",
            "Loss: \n",
            "0.0002011413317898421\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523827]\n",
            " [0.87908595]\n",
            " [0.88541116]]\n",
            "\n",
            "Loss: \n",
            "0.000201079931001384\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523773]\n",
            " [0.87908165]\n",
            " [0.88541501]]\n",
            "\n",
            "Loss: \n",
            "0.0002010185997840205\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052372 ]\n",
            " [0.87907734]\n",
            " [0.88541886]]\n",
            "\n",
            "Loss: \n",
            "0.000200957338035959\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523666]\n",
            " [0.87907304]\n",
            " [0.88542271]]\n",
            "\n",
            "Loss: \n",
            "0.0002008961456555805\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523613]\n",
            " [0.87906874]\n",
            " [0.88542655]]\n",
            "\n",
            "Loss: \n",
            "0.00020083502254144808\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052356 ]\n",
            " [0.87906444]\n",
            " [0.88543039]]\n",
            "\n",
            "Loss: \n",
            "0.00020077396859228768\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523507]\n",
            " [0.87906015]\n",
            " [0.88543422]]\n",
            "\n",
            "Loss: \n",
            "0.00020071298370701507\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523454]\n",
            " [0.87905586]\n",
            " [0.88543806]]\n",
            "\n",
            "Loss: \n",
            "0.00020065206778471613\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.905234  ]\n",
            " [0.87905158]\n",
            " [0.88544189]]\n",
            "\n",
            "Loss: \n",
            "0.00020059122072464819\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523347]\n",
            " [0.8790473 ]\n",
            " [0.88544571]]\n",
            "\n",
            "Loss: \n",
            "0.00020053044242624848\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523295]\n",
            " [0.87904302]\n",
            " [0.88544954]]\n",
            "\n",
            "Loss: \n",
            "0.00020046973278912245\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523242]\n",
            " [0.87903875]\n",
            " [0.88545336]]\n",
            "\n",
            "Loss: \n",
            "0.0002004090917130524\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523189]\n",
            " [0.87903447]\n",
            " [0.88545718]]\n",
            "\n",
            "Loss: \n",
            "0.00020034851909799307\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523136]\n",
            " [0.87903021]\n",
            " [0.88546099]]\n",
            "\n",
            "Loss: \n",
            "0.00020028801484407296\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523083]\n",
            " [0.87902594]\n",
            " [0.88546481]]\n",
            "\n",
            "Loss: \n",
            "0.0002002275788515942\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90523031]\n",
            " [0.87902168]\n",
            " [0.88546862]]\n",
            "\n",
            "Loss: \n",
            "0.00020016721102102963\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522978]\n",
            " [0.87901743]\n",
            " [0.88547242]]\n",
            "\n",
            "Loss: \n",
            "0.0002001069112530222\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522926]\n",
            " [0.87901317]\n",
            " [0.88547623]]\n",
            "\n",
            "Loss: \n",
            "0.00020004667944839084\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522873]\n",
            " [0.87900892]\n",
            " [0.88548003]]\n",
            "\n",
            "Loss: \n",
            "0.0001999865155081189\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522821]\n",
            " [0.87900468]\n",
            " [0.88548383]]\n",
            "\n",
            "Loss: \n",
            "0.0001999264193333753\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522769]\n",
            " [0.87900043]\n",
            " [0.88548762]]\n",
            "\n",
            "Loss: \n",
            "0.00019986639082548196\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522717]\n",
            " [0.87899619]\n",
            " [0.88549142]]\n",
            "\n",
            "Loss: \n",
            "0.00019980642988594094\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522664]\n",
            " [0.87899196]\n",
            " [0.88549521]]\n",
            "\n",
            "Loss: \n",
            "0.00019974653641641966\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522612]\n",
            " [0.87898772]\n",
            " [0.88549899]]\n",
            "\n",
            "Loss: \n",
            "0.0001996867103187634\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052256 ]\n",
            " [0.87898349]\n",
            " [0.88550278]]\n",
            "\n",
            "Loss: \n",
            "0.00019962695149497748\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522508]\n",
            " [0.87897927]\n",
            " [0.88550656]]\n",
            "\n",
            "Loss: \n",
            "0.00019956725984723997\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522457]\n",
            " [0.87897505]\n",
            " [0.88551034]]\n",
            "\n",
            "Loss: \n",
            "0.00019950763527789805\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522405]\n",
            " [0.87897083]\n",
            " [0.88551411]]\n",
            "\n",
            "Loss: \n",
            "0.00019944807768946813\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522353]\n",
            " [0.87896661]\n",
            " [0.88551789]]\n",
            "\n",
            "Loss: \n",
            "0.00019938858698462876\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522301]\n",
            " [0.8789624 ]\n",
            " [0.88552166]]\n",
            "\n",
            "Loss: \n",
            "0.00019932916306623653\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052225 ]\n",
            " [0.87895819]\n",
            " [0.88552542]]\n",
            "\n",
            "Loss: \n",
            "0.00019926980583730448\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522198]\n",
            " [0.87895398]\n",
            " [0.88552919]]\n",
            "\n",
            "Loss: \n",
            "0.0001992105152010226\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522147]\n",
            " [0.87894978]\n",
            " [0.88553295]]\n",
            "\n",
            "Loss: \n",
            "0.0001991512910607369\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522095]\n",
            " [0.87894558]\n",
            " [0.88553671]]\n",
            "\n",
            "Loss: \n",
            "0.00019909213331996842\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90522044]\n",
            " [0.87894139]\n",
            " [0.88554046]]\n",
            "\n",
            "Loss: \n",
            "0.00019903304188240202\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521992]\n",
            " [0.8789372 ]\n",
            " [0.88554422]]\n",
            "\n",
            "Loss: \n",
            "0.00019897401665188518\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521941]\n",
            " [0.87893301]\n",
            " [0.88554797]]\n",
            "\n",
            "Loss: \n",
            "0.00019891505753243552\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052189 ]\n",
            " [0.87892882]\n",
            " [0.88555172]]\n",
            "\n",
            "Loss: \n",
            "0.00019885616442823415\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521839]\n",
            " [0.87892464]\n",
            " [0.88555546]]\n",
            "\n",
            "Loss: \n",
            "0.0001987973372436218\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521788]\n",
            " [0.87892046]\n",
            " [0.8855592 ]]\n",
            "\n",
            "Loss: \n",
            "0.0001987385758831105\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521737]\n",
            " [0.87891629]\n",
            " [0.88556294]]\n",
            "\n",
            "Loss: \n",
            "0.000198679880251377\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521686]\n",
            " [0.87891212]\n",
            " [0.88556668]]\n",
            "\n",
            "Loss: \n",
            "0.00019862125025325514\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521635]\n",
            " [0.87890795]\n",
            " [0.88557041]]\n",
            "\n",
            "Loss: \n",
            "0.00019856268579374387\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521584]\n",
            " [0.87890378]\n",
            " [0.88557414]]\n",
            "\n",
            "Loss: \n",
            "0.00019850418677801293\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521533]\n",
            " [0.87889962]\n",
            " [0.88557787]]\n",
            "\n",
            "Loss: \n",
            "0.0001984457531113867\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521483]\n",
            " [0.87889546]\n",
            " [0.8855816 ]]\n",
            "\n",
            "Loss: \n",
            "0.0001983873846993521\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521432]\n",
            " [0.87889131]\n",
            " [0.88558532]]\n",
            "\n",
            "Loss: \n",
            "0.00019832908144756402\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521382]\n",
            " [0.87888716]\n",
            " [0.88558904]]\n",
            "\n",
            "Loss: \n",
            "0.00019827084326183252\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521331]\n",
            " [0.87888301]\n",
            " [0.88559276]]\n",
            "\n",
            "Loss: \n",
            "0.00019821267004813811\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521281]\n",
            " [0.87887886]\n",
            " [0.88559647]]\n",
            "\n",
            "Loss: \n",
            "0.00019815456171261365\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052123 ]\n",
            " [0.87887472]\n",
            " [0.88560018]]\n",
            "\n",
            "Loss: \n",
            "0.00019809651816155583\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052118 ]\n",
            " [0.87887058]\n",
            " [0.88560389]]\n",
            "\n",
            "Loss: \n",
            "0.0001980385393014273\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052113 ]\n",
            " [0.87886645]\n",
            " [0.8856076 ]]\n",
            "\n",
            "Loss: \n",
            "0.00019798062503884119\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052108 ]\n",
            " [0.87886232]\n",
            " [0.8856113 ]]\n",
            "\n",
            "Loss: \n",
            "0.0001979227752805792\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90521029]\n",
            " [0.87885819]\n",
            " [0.885615  ]]\n",
            "\n",
            "Loss: \n",
            "0.00019786498993357445\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520979]\n",
            " [0.87885407]\n",
            " [0.8856187 ]]\n",
            "\n",
            "Loss: \n",
            "0.0001978072689049303\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520929]\n",
            " [0.87884994]\n",
            " [0.88562239]]\n",
            "\n",
            "Loss: \n",
            "0.00019774961210190033\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520879]\n",
            " [0.87884583]\n",
            " [0.88562608]]\n",
            "\n",
            "Loss: \n",
            "0.0001976920194319036\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052083 ]\n",
            " [0.87884171]\n",
            " [0.88562977]]\n",
            "\n",
            "Loss: \n",
            "0.00019763449080250717\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052078 ]\n",
            " [0.8788376 ]\n",
            " [0.88563346]]\n",
            "\n",
            "Loss: \n",
            "0.0001975770261214503\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052073 ]\n",
            " [0.87883349]\n",
            " [0.88563714]]\n",
            "\n",
            "Loss: \n",
            "0.00019751962529661714\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052068 ]\n",
            " [0.87882939]\n",
            " [0.88564083]]\n",
            "\n",
            "Loss: \n",
            "0.00019746228823605886\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520631]\n",
            " [0.87882528]\n",
            " [0.8856445 ]]\n",
            "\n",
            "Loss: \n",
            "0.00019740501484797831\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520581]\n",
            " [0.87882119]\n",
            " [0.88564818]]\n",
            "\n",
            "Loss: \n",
            "0.00019734780504073712\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520532]\n",
            " [0.87881709]\n",
            " [0.88565185]]\n",
            "\n",
            "Loss: \n",
            "0.00019729065872285106\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520482]\n",
            " [0.878813  ]\n",
            " [0.88565552]]\n",
            "\n",
            "Loss: \n",
            "0.00019723357580300228\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520433]\n",
            " [0.87880891]\n",
            " [0.88565919]]\n",
            "\n",
            "Loss: \n",
            "0.00019717655619001461\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520383]\n",
            " [0.87880483]\n",
            " [0.88566285]]\n",
            "\n",
            "Loss: \n",
            "0.00019711959979287546\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520334]\n",
            " [0.87880074]\n",
            " [0.88566652]]\n",
            "\n",
            "Loss: \n",
            "0.0001970627065207283\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520285]\n",
            " [0.87879666]\n",
            " [0.88567018]]\n",
            "\n",
            "Loss: \n",
            "0.00019700587628287055\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520236]\n",
            " [0.87879259]\n",
            " [0.88567383]]\n",
            "\n",
            "Loss: \n",
            "0.00019694910898875048\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520187]\n",
            " [0.87878852]\n",
            " [0.88567749]]\n",
            "\n",
            "Loss: \n",
            "0.0001968924045479795\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520138]\n",
            " [0.87878445]\n",
            " [0.88568114]]\n",
            "\n",
            "Loss: \n",
            "0.00019683576287031382\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90520089]\n",
            " [0.87878038]\n",
            " [0.88568479]]\n",
            "\n",
            "Loss: \n",
            "0.00019677918386566817\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.9052004 ]\n",
            " [0.87877632]\n",
            " [0.88568843]]\n",
            "\n",
            "Loss: \n",
            "0.00019672266744411306\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90519991]\n",
            " [0.87877226]\n",
            " [0.88569208]]\n",
            "\n",
            "Loss: \n",
            "0.00019666621351586893\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90519942]\n",
            " [0.8787682 ]\n",
            " [0.88569572]]\n",
            "\n",
            "Loss: \n",
            "0.00019660982199130864\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90519894]\n",
            " [0.87876415]\n",
            " [0.88569935]]\n",
            "\n",
            "Loss: \n",
            "0.00019655349278096013\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90519845]\n",
            " [0.8787601 ]\n",
            " [0.88570299]]\n",
            "\n",
            "Loss: \n",
            "0.00019649722579550568\n",
            "\n",
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "\n",
            "Predicted Output: \n",
            "[[0.90519796]\n",
            " [0.87875606]\n",
            " [0.88570662]]\n",
            "\n",
            "Loss: \n",
            "0.00019644102094577135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Write a program to implement the naïve Bayesian classifier for a sample training data set stored as a .CSV file. Compute the accuracy of the classifier, considering few test data sets. **"
      ],
      "metadata": {
        "id": "R-hzoVcVP156"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libarities\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# load data from CSV\n",
        "data = pd.read_csv('tennisdata.csv')\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "# Convert then in numbers\n",
        "le_outlook = LabelEncoder()\n",
        "X.Outlook = le_outlook.fit_transform(X.Outlook)\n",
        "\n",
        "le_Temperature = LabelEncoder()\n",
        "X.Temperature = le_Temperature.fit_transform(X.Temperature)\n",
        "\n",
        "le_Humidity = LabelEncoder()\n",
        "X.Humidity = le_Humidity.fit_transform(X.Humidity)\n",
        "\n",
        "le_Windy = LabelEncoder()\n",
        "X.Windy = le_Windy.fit_transform(X.Windy)\n",
        "le_PlayTennis = LabelEncoder()\n",
        "y = le_PlayTennis.fit_transform(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20)\n",
        "\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train,y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy is:\",accuracy_score(classifier.predict(X_test),y_test))"
      ],
      "metadata": {
        "id": "W1WUvQWsH21r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "9d4686ac-fcb2-454e-b85e-6331b1ef13ad"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Assuming a set of documents that need to be classified, use the naïve Bayesian Classifier model to perform this task. Built-in Java classes/API can be used to write the program. Calculate the accuracy, precision, and recall for your data set.**"
      ],
      "metadata": {
        "id": "YXqydGkYQ2oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msg = pd.read_csv('document.csv', names=['message', 'label'])\n",
        "print(\"Total Instances of Dataset: \", msg.shape[0])\n",
        "msg['labelnum'] = msg.label.map({'pos': 1, 'neg': 0})\n",
        "X = msg.message\n",
        "y = msg.labelnum\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_v = CountVectorizer()\n",
        "Xtrain_dm = count_v.fit_transform(Xtrain)\n",
        "Xtest_dm = count_v.transform(Xtest)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(Xtrain_dm, ytrain)\n",
        "pred = clf.predict(Xtest_dm)\n",
        "\n",
        "for doc, p in zip(Xtrain, pred):\n",
        "    p = 'pos' if p == 1 else 'neg'\n",
        "    print(\"%s -> %s\" % (doc, p))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "print('Accuracy Metrics: \\n')\n",
        "print('Accuracy: ', accuracy_score(ytest, pred))\n",
        "print('Recall: ', recall_score(ytest, pred))\n",
        "print('Precision: ', precision_score(ytest, pred))\n",
        "print('Confusion Matrix: \\n', confusion_matrix(ytest, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EJUJkENrRF5R",
        "outputId": "8428fc0a-7679-40d7-d541-44cdab302880"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Instances of Dataset:  18\n",
            "This is an awesome place -> neg\n",
            "This is my best work -> pos\n",
            "I love to dance -> neg\n",
            "I feel very good about these beers -> pos\n",
            "I am tired of this stuff -> neg\n",
            "Accuracy Metrics: \n",
            "\n",
            "Accuracy:  0.8\n",
            "Recall:  1.0\n",
            "Precision:  0.5\n",
            "Confusion Matrix: \n",
            " [[3 1]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Write a program to construct a Bayesian network considering medical data. Use this model to demonstrate the diagnosis of heart patients using standard Heart Disease Data Set. You can use Java/Python ML library classes/API. **"
      ],
      "metadata": {
        "id": "kvCq6FIwRhfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"heartdisease.csv\")\n",
        "heart_disease=pd.DataFrame(data)\n",
        "\n",
        "from pgmpy.models import BayesianModel\n",
        "model=BayesianModel([\n",
        "('age','Lifestyle'),\n",
        "('Gender','Lifestyle'),\n",
        "('Family','heartdisease'),\n",
        "('diet','cholestrol'),\n",
        "('Lifestyle','diet'),\n",
        "('cholestrol','heartdisease'),\n",
        "('diet','cholestrol')\n",
        "])\n",
        "\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "model.fit(heart_disease, estimator=MaximumLikelihoodEstimator)\n",
        "\n",
        "from pgmpy.inference import VariableElimination\n",
        "HeartDisease_infer = VariableElimination(model)\n",
        "\n",
        "print('For age Enter { SuperSeniorCitizen:0, SeniorCitizen:1, MiddleAged:2, Youth:3, Teen:4 }')\n",
        "print('For Gender Enter { Male:0, Female:1 }')\n",
        "print('For Family History Enter { yes:1, No:0 }')\n",
        "print('For diet Enter { High:0, Medium:1 }')\n",
        "print('For lifeStyle Enter { Athlete:0, Active:1, Moderate:2, Sedentary:3 }')\n",
        "print('For cholesterol Enter { High:0, BorderLine:1, Normal:2 }')\n",
        "\n",
        "q = HeartDisease_infer.query(variables=['heartdisease'], evidence={\n",
        "    'age':int(input('Enter age :')),\n",
        "    'Gender':int(input('Enter Gender :')),\n",
        "    'Family':int(input('Enter Family history :')),\n",
        "    'diet':int(input('Enter diet :')),\n",
        "    'Lifestyle':int(input('Enter Lifestyle :')),\n",
        "    'cholestrol':int(input('Enter cholestrol :'))\n",
        "    })\n",
        "\n",
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BFKUumkLSyBT",
        "outputId": "b5e1544f-8750-4e08-bb5f-4599c18b72dd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For age Enter { SuperSeniorCitizen:0, SeniorCitizen:1, MiddleAged:2, Youth:3, Teen:4 }\n",
            "For Gender Enter { Male:0, Female:1 }\n",
            "For Family History Enter { yes:1, No:0 }\n",
            "For diet Enter { High:0, Medium:1 }\n",
            "For lifeStyle Enter { Athlete:0, Active:1, Moderate:2, Sedentary:3 }\n",
            "For cholesterol Enter { High:0, BorderLine:1, Normal:2 }\n",
            "Enter age :1\n",
            "Enter Gender :1\n",
            "Enter Family history :0\n",
            "Enter diet :1\n",
            "Enter Lifestyle :0\n",
            "Enter cholestrol :1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n",
            "WARNING:pgmpy:BayesianModel has been renamed to BayesianNetwork. Please use BayesianNetwork class, BayesianModel will be removed in future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------------------+\n",
            "| heartdisease    |   phi(heartdisease) |\n",
            "+=================+=====================+\n",
            "| heartdisease(0) |              0.0000 |\n",
            "+-----------------+---------------------+\n",
            "| heartdisease(1) |              1.0000 |\n",
            "+-----------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Apply EM algorithm to cluster a set of data stored in a .CSV file. Use the same data et for clustering using k-Means algorithm. Compare the results of these two algorithms and comment on the quality of clustering. You can add Java/python ML library classes/API in the program**"
      ],
      "metadata": {
        "id": "OyRbPQcbTKzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.datasets import load_iris\n",
        "import sklearn.metrics as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dataset=load_iris()"
      ],
      "metadata": {
        "id": "AOLQSwmWTLh4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=pd.DataFrame(dataset.data)\n",
        "X.columns=['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
        "y=pd.DataFrame(dataset.target)\n",
        "y.columns=['Targets']\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "colormap=np.array(['red','lime','black'])\n",
        "\n",
        "# REAL PLOT\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y.Targets],s=40)\n",
        "plt.title('Real')\n",
        "\n",
        "# K-PLOT\n",
        "plt.subplot(1,3,2)\n",
        "model=KMeans(n_clusters=3)\n",
        "model.fit(X)\n",
        "predY=np.choose(model.labels_,[0,1,2]).astype(np.int64)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[predY],s=40)\n",
        "plt.title('KMeans')\n",
        "\n",
        "# GMM PLOT\n",
        "scaler=preprocessing.StandardScaler()\n",
        "scaler.fit(X)\n",
        "xsa=scaler.transform(X)\n",
        "xs=pd.DataFrame(xsa,columns=X.columns)\n",
        "gmm=GaussianMixture(n_components=3)\n",
        "gmm.fit(xs)\n",
        "y_cluster_gmm=gmm.predict(xs)\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(X.Petal_Length,X.Petal_Width,c=colormap[y_cluster_gmm],s=40)\n",
        "plt.title('GMM Classification')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "thmKHzFGU-4B",
        "outputId": "606898d0-a34e-471c-b655-6775e66b8dac"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'GMM Classification')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x700 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.Write a program to implement k-Nearest Neighbour algorithm to classify the iris data set. Print both correct and wrong predictions. Java/Python ML library classes can be used for this problem.**"
      ],
      "metadata": {
        "id": "S9U7JIIBVT2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "dataset=load_iris()\n",
        "X_train,X_test,y_train,y_test=train_test_split(dataset[\"data\"],dataset[\"target\"],random_state=0)\n",
        "\n",
        "kn=KNeighborsClassifier(n_neighbors=1)\n",
        "kn.fit(X_train,y_train)\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    x=X_test[i]\n",
        "    x_new=np.array([x])\n",
        "    prediction=kn.predict(x_new)\n",
        "    print(\"TARGET=\",y_test[i],dataset[\"target_names\"][y_test[i]],\"PREDICTED=\",prediction,dataset[\"target_names\"][prediction])\n",
        "print(kn.score(X_test,y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "19yINJmmVo9n",
        "outputId": "623f3233-eb8b-4c1c-e223-c3bc0b8df679"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 2 virginica PREDICTED= [2] ['virginica']\n",
            "TARGET= 1 versicolor PREDICTED= [1] ['versicolor']\n",
            "TARGET= 0 setosa PREDICTED= [0] ['setosa']\n",
            "TARGET= 1 versicolor PREDICTED= [2] ['virginica']\n",
            "0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Implement the non-parametric Locally Weighted Regression algorithm in order to fit data points. Select appropriate data set for your experiment and draw graphs.**"
      ],
      "metadata": {
        "id": "f_yBr2CBWkol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "from scipy import linalg\n",
        "\n",
        "def lowess(x, y, f, iterations):\n",
        "    n = len(x)\n",
        "    r = int(ceil(f * n))\n",
        "    h = [np.sort(np.abs(x - x[i]))[r] for i in range(n)]\n",
        "    w = np.clip(np.abs((x[:, None] - x[None, :]) / h), 0.0, 1.0)\n",
        "    w = (1 - w ** 3) ** 3\n",
        "    yest = np.zeros(n)\n",
        "    delta = np.ones(n)\n",
        "    for iteration in range(iterations):\n",
        "        for i in range(n):\n",
        "            weights = delta * w[:, i]\n",
        "            b = np.array([np.sum(weights * y), np.sum(weights * y * x)])\n",
        "            A = np.array([[np.sum(weights), np.sum(weights * x)],[np.sum(weights * x), np.sum(weights * x * x)]])\n",
        "            beta = linalg.solve(A, b)\n",
        "            yest[i] = beta[0] + beta[1] * x[i]\n",
        "\n",
        "        residuals = y - yest\n",
        "        s = np.median(np.abs(residuals))\n",
        "        delta = np.clip(residuals / (6.0 * s), -1, 1)\n",
        "        delta = (1 - delta ** 2) ** 2\n",
        "\n",
        "    return yest\n",
        "\n",
        "\n",
        "n = 100\n",
        "x = np.linspace(0, 2 * math.pi, n)\n",
        "y = np.sin(x) + 0.3 * np.random.randn(n)\n",
        "f =0.25\n",
        "iterations=3\n",
        "yest = lowess(x, y, f, iterations)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x,y,\"r.\")\n",
        "plt.plot(x,yest,\"b-\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "dfKksWCrW0IT",
        "outputId": "71c3eee3-7faf-46bd-dea1-806e1b3c787a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79cf10396f20>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}